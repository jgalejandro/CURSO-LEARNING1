{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "v5Tkx9SoZK4b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HmTrzSzwaFgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd154a20-f4f2-46e2-c94a-3e3e67ad851c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (validation_images, validation_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images2, train_labels2), (validation_images2, validation_labels2) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ2NJd6yG2T0",
        "outputId": "976b61b7-7b4a-4c43-f20b-2d7f0f0208f2"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "_Xrb2BXCaSPy"
      },
      "outputs": [],
      "source": [
        "#normalizacion de datos\n",
        "media = np.mean(train_images,axis=0)\n",
        "desvio = np.std(train_images,axis=0)\n",
        "train_images = (train_images - media)/desvio\n",
        "numero_muestras = train_images.shape[0] \n",
        "area_imagen = train_images.shape[1]*train_images.shape[2] \n",
        "\n",
        "validation_images = (validation_images - media)/desvio\n",
        "\n",
        "validation_images2 = (validation_images2 - media)/desvio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE4_UgX3ayVf",
        "outputId": "b7c3540b-1d3b-448e-ca30-a2ab0a892d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 150)               117750    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 784)               118384    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 28, 28)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 236,134\n",
            "Trainable params: 236,134\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.9133 - val_loss: 2.4399\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 2.3995 - val_loss: 2.1062\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 2.0680 - val_loss: 1.8928\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 1.8568 - val_loss: 1.7535\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 1.7195 - val_loss: 1.6581\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 1.6259 - val_loss: 1.5885\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 1.5577 - val_loss: 1.5343\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 1.5047 - val_loss: 1.4893\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 1.4608 - val_loss: 1.4499\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 1.4222 - val_loss: 1.4136\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 1.3867 - val_loss: 1.3793\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 1.3530 - val_loss: 1.3464\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 1.3207 - val_loss: 1.3148\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 1.2896 - val_loss: 1.2846\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 1.2600 - val_loss: 1.2561\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 1.2320 - val_loss: 1.2294\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 1.2056 - val_loss: 1.2044\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 1.1810 - val_loss: 1.1809\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 1.1578 - val_loss: 1.1587\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 1.1359 - val_loss: 1.1376\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 1.1151 - val_loss: 1.1175\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 1.0952 - val_loss: 1.0982\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 1.0762 - val_loss: 1.0795\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 1.0577 - val_loss: 1.0614\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 1.0397 - val_loss: 1.0435\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 1.0220 - val_loss: 1.0259\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 1.0045 - val_loss: 1.0086\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.9872 - val_loss: 0.9916\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.9703 - val_loss: 0.9750\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.9537 - val_loss: 0.9589\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 0.9376 - val_loss: 0.9433\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 0.9220 - val_loss: 0.9282\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.9070 - val_loss: 0.9137\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.8925 - val_loss: 0.8997\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 0.8785 - val_loss: 0.8861\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.8650 - val_loss: 0.8731\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.8520 - val_loss: 0.8606\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.8395 - val_loss: 0.8486\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.8274 - val_loss: 0.8370\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.8159 - val_loss: 0.8258\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.8047 - val_loss: 0.8151\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.7940 - val_loss: 0.8047\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.7836 - val_loss: 0.7947\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.7736 - val_loss: 0.7851\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.7640 - val_loss: 0.7758\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.7548 - val_loss: 0.7668\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.7458 - val_loss: 0.7582\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.7372 - val_loss: 0.7499\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.7289 - val_loss: 0.7419\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.7209 - val_loss: 0.7341\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.7131 - val_loss: 0.7265\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.7055 - val_loss: 0.7192\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.6982 - val_loss: 0.7121\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.6911 - val_loss: 0.7052\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.6841 - val_loss: 0.6985\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.6774 - val_loss: 0.6919\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.6709 - val_loss: 0.6856\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.6645 - val_loss: 0.6794\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.6583 - val_loss: 0.6734\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.6523 - val_loss: 0.6675\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.6464 - val_loss: 0.6617\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.6406 - val_loss: 0.6561\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.6350 - val_loss: 0.6506\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.6295 - val_loss: 0.6452\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.6242 - val_loss: 0.6400\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.6189 - val_loss: 0.6348\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.6138 - val_loss: 0.6298\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.6088 - val_loss: 0.6249\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.6039 - val_loss: 0.6201\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.5991 - val_loss: 0.6154\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.5944 - val_loss: 0.6107\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.5898 - val_loss: 0.6062\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.5853 - val_loss: 0.6018\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.5809 - val_loss: 0.5975\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.5766 - val_loss: 0.5932\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.5723 - val_loss: 0.5891\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.5682 - val_loss: 0.5850\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.5641 - val_loss: 0.5810\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.5601 - val_loss: 0.5771\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.5562 - val_loss: 0.5733\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.5524 - val_loss: 0.5695\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.5486 - val_loss: 0.5658\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.5449 - val_loss: 0.5622\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.5413 - val_loss: 0.5586\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.5378 - val_loss: 0.5551\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.5343 - val_loss: 0.5517\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.5309 - val_loss: 0.5483\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.5276 - val_loss: 0.5450\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.5243 - val_loss: 0.5418\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.5211 - val_loss: 0.5387\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.5180 - val_loss: 0.5355\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.5149 - val_loss: 0.5325\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.5118 - val_loss: 0.5295\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.5089 - val_loss: 0.5266\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.5059 - val_loss: 0.5237\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.5031 - val_loss: 0.5209\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.5003 - val_loss: 0.5181\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.4975 - val_loss: 0.5154\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.4948 - val_loss: 0.5127\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.4921 - val_loss: 0.5101\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.4895 - val_loss: 0.5075\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.4870 - val_loss: 0.5050\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4844 - val_loss: 0.5025\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.4819 - val_loss: 0.5000\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.4795 - val_loss: 0.4976\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.4771 - val_loss: 0.4952\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.4747 - val_loss: 0.4929\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.4724 - val_loss: 0.4906\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.4701 - val_loss: 0.4883\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4679 - val_loss: 0.4861\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.4657 - val_loss: 0.4839\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4635 - val_loss: 0.4817\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4614 - val_loss: 0.4796\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.4593 - val_loss: 0.4774\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4572 - val_loss: 0.4754\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.4552 - val_loss: 0.4733\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.4532 - val_loss: 0.4713\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4512 - val_loss: 0.4694\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4492 - val_loss: 0.4674\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.4473 - val_loss: 0.4655\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.4454 - val_loss: 0.4636\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.4436 - val_loss: 0.4617\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4417 - val_loss: 0.4599\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.4399 - val_loss: 0.4581\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4381 - val_loss: 0.4563\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.4363 - val_loss: 0.4545\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4346 - val_loss: 0.4528\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.4329 - val_loss: 0.4511\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4312 - val_loss: 0.4494\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.4295 - val_loss: 0.4477\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.4279 - val_loss: 0.4460\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.4263 - val_loss: 0.4444\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.4246 - val_loss: 0.4428\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.4231 - val_loss: 0.4412\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.4215 - val_loss: 0.4396\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.4199 - val_loss: 0.4380\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4184 - val_loss: 0.4365\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.4169 - val_loss: 0.4349\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.4154 - val_loss: 0.4334\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.4139 - val_loss: 0.4319\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.4125 - val_loss: 0.4305\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4110 - val_loss: 0.4290\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.4096 - val_loss: 0.4276\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.4082 - val_loss: 0.4261\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.4068 - val_loss: 0.4247\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4054 - val_loss: 0.4233\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.4040 - val_loss: 0.4219\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.4027 - val_loss: 0.4205\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4013 - val_loss: 0.4192\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.4000 - val_loss: 0.4178\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.3987 - val_loss: 0.4165\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.3974 - val_loss: 0.4152\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.3961 - val_loss: 0.4139\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.3948 - val_loss: 0.4126\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.3936 - val_loss: 0.4113\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.3923 - val_loss: 0.4100\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.3911 - val_loss: 0.4087\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.3899 - val_loss: 0.4075\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.3886 - val_loss: 0.4063\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3874 - val_loss: 0.4050\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.3862 - val_loss: 0.4038\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.3851 - val_loss: 0.4026\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.3839 - val_loss: 0.4014\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.3827 - val_loss: 0.4002\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.3816 - val_loss: 0.3990\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.3804 - val_loss: 0.3978\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.3793 - val_loss: 0.3967\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.3782 - val_loss: 0.3955\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.3770 - val_loss: 0.3944\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.3759 - val_loss: 0.3932\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.3748 - val_loss: 0.3921\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.3737 - val_loss: 0.3910\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.3727 - val_loss: 0.3899\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.3716 - val_loss: 0.3887\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3705 - val_loss: 0.3876\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.3695 - val_loss: 0.3865\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3684 - val_loss: 0.3854\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.3674 - val_loss: 0.3844\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.3663 - val_loss: 0.3833\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.3653 - val_loss: 0.3822\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.3643 - val_loss: 0.3812\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3633 - val_loss: 0.3801\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.3623 - val_loss: 0.3791\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.3613 - val_loss: 0.3780\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.3603 - val_loss: 0.3770\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.3593 - val_loss: 0.3760\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.3583 - val_loss: 0.3750\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.3573 - val_loss: 0.3740\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.3564 - val_loss: 0.3730\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.3554 - val_loss: 0.3720\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3545 - val_loss: 0.3710\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.3535 - val_loss: 0.3700\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.3526 - val_loss: 0.3690\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.3517 - val_loss: 0.3680\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.3507 - val_loss: 0.3670\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.3498 - val_loss: 0.3661\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.3489 - val_loss: 0.3651\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.3480 - val_loss: 0.3642\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.3471 - val_loss: 0.3632\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.3462 - val_loss: 0.3623\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3453 - val_loss: 0.3614\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.3444 - val_loss: 0.3604\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.3435 - val_loss: 0.3595\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3426 - val_loss: 0.3586\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.3418 - val_loss: 0.3577\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.3409 - val_loss: 0.3568\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.3400 - val_loss: 0.3559\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3392 - val_loss: 0.3550\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.3383 - val_loss: 0.3541\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.3375 - val_loss: 0.3532\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.3366 - val_loss: 0.3523\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3358 - val_loss: 0.3514\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.3350 - val_loss: 0.3506\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.3341 - val_loss: 0.3497\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3333 - val_loss: 0.3488\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.3325 - val_loss: 0.3480\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.3317 - val_loss: 0.3471\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.3309 - val_loss: 0.3462\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.3301 - val_loss: 0.3454\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.3293 - val_loss: 0.3446\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.3285 - val_loss: 0.3437\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.3277 - val_loss: 0.3429\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.3269 - val_loss: 0.3421\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.3261 - val_loss: 0.3412\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.3253 - val_loss: 0.3404\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.3246 - val_loss: 0.3396\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.3238 - val_loss: 0.3388\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.3230 - val_loss: 0.3380\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.3223 - val_loss: 0.3372\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.3215 - val_loss: 0.3364\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.3208 - val_loss: 0.3356\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.3200 - val_loss: 0.3348\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.3193 - val_loss: 0.3340\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.3185 - val_loss: 0.3332\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.3178 - val_loss: 0.3324\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.3170 - val_loss: 0.3317\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.3163 - val_loss: 0.3309\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.3156 - val_loss: 0.3301\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.3149 - val_loss: 0.3293\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.3141 - val_loss: 0.3286\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.3134 - val_loss: 0.3278\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.3127 - val_loss: 0.3271\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.3120 - val_loss: 0.3263\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.3113 - val_loss: 0.3256\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3106 - val_loss: 0.3248\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.3099 - val_loss: 0.3241\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.3092 - val_loss: 0.3233\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.3085 - val_loss: 0.3226\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3078 - val_loss: 0.3219\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.3071 - val_loss: 0.3212\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3065 - val_loss: 0.3204\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.3058 - val_loss: 0.3197\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3051 - val_loss: 0.3190\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.3044 - val_loss: 0.3183\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3038 - val_loss: 0.3176\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.3031 - val_loss: 0.3169\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.3024 - val_loss: 0.3162\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.3018 - val_loss: 0.3155\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.3011 - val_loss: 0.3148\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.3005 - val_loss: 0.3141\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.2998 - val_loss: 0.3134\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.2992 - val_loss: 0.3127\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.2985 - val_loss: 0.3120\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2979 - val_loss: 0.3114\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2973 - val_loss: 0.3107\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.2966 - val_loss: 0.3100\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.2960 - val_loss: 0.3093\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2954 - val_loss: 0.3087\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.2947 - val_loss: 0.3080\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.2941 - val_loss: 0.3073\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2935 - val_loss: 0.3067\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.2929 - val_loss: 0.3060\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.2923 - val_loss: 0.3053\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.2916 - val_loss: 0.3047\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2910 - val_loss: 0.3040\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.2904 - val_loss: 0.3034\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.2898 - val_loss: 0.3028\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.2892 - val_loss: 0.3021\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.2886 - val_loss: 0.3015\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.2880 - val_loss: 0.3008\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.2874 - val_loss: 0.3002\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2868 - val_loss: 0.2996\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.2863 - val_loss: 0.2989\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2857 - val_loss: 0.2983\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2851 - val_loss: 0.2977\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.2845 - val_loss: 0.2971\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.2839 - val_loss: 0.2965\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2834 - val_loss: 0.2958\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2828 - val_loss: 0.2952\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.2822 - val_loss: 0.2946\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2816 - val_loss: 0.2940\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.2811 - val_loss: 0.2934\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.2805 - val_loss: 0.2928\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2799 - val_loss: 0.2922\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2794 - val_loss: 0.2916\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.2788 - val_loss: 0.2910\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.2783 - val_loss: 0.2904\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.2777 - val_loss: 0.2899\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.2772 - val_loss: 0.2893\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.2766 - val_loss: 0.2887\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2761 - val_loss: 0.2881\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.2755 - val_loss: 0.2875\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2750 - val_loss: 0.2869\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2745 - val_loss: 0.2864\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.2739 - val_loss: 0.2858\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.2734 - val_loss: 0.2852\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2729 - val_loss: 0.2847\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.2723 - val_loss: 0.2841\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2718 - val_loss: 0.2835\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2713 - val_loss: 0.2830\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.2708 - val_loss: 0.2824\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.2702 - val_loss: 0.2819\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2697 - val_loss: 0.2813\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2692 - val_loss: 0.2808\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2687 - val_loss: 0.2802\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.2682 - val_loss: 0.2797\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.2677 - val_loss: 0.2791\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.2672 - val_loss: 0.2786\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.2666 - val_loss: 0.2781\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.2661 - val_loss: 0.2775\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2656 - val_loss: 0.2770\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2651 - val_loss: 0.2764\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.2646 - val_loss: 0.2759\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2641 - val_loss: 0.2754\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2637 - val_loss: 0.2749\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2632 - val_loss: 0.2743\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2627 - val_loss: 0.2738\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.2622 - val_loss: 0.2733\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.2617 - val_loss: 0.2728\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.2612 - val_loss: 0.2723\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.2607 - val_loss: 0.2718\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.2602 - val_loss: 0.2712\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2598 - val_loss: 0.2707\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.2593 - val_loss: 0.2702\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.2588 - val_loss: 0.2697\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.2583 - val_loss: 0.2692\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.2579 - val_loss: 0.2687\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.2574 - val_loss: 0.2682\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 0.2569 - val_loss: 0.2677\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.2565 - val_loss: 0.2672\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2560 - val_loss: 0.2667\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.2555 - val_loss: 0.2662\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.2551 - val_loss: 0.2657\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.2546 - val_loss: 0.2652\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.2542 - val_loss: 0.2648\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.2537 - val_loss: 0.2643\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.2533 - val_loss: 0.2638\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.2528 - val_loss: 0.2633\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2523 - val_loss: 0.2628\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2519 - val_loss: 0.2624\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.2515 - val_loss: 0.2619\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.2510 - val_loss: 0.2614\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.2506 - val_loss: 0.2609\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.2501 - val_loss: 0.2605\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.2497 - val_loss: 0.2600\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.2492 - val_loss: 0.2595\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.2488 - val_loss: 0.2590\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.2484 - val_loss: 0.2585\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.2479 - val_loss: 0.2581\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.2475 - val_loss: 0.2576\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2471 - val_loss: 0.2571\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2466 - val_loss: 0.2567\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.2462 - val_loss: 0.2562\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2458 - val_loss: 0.2558\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2454 - val_loss: 0.2553\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2449 - val_loss: 0.2549\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2445 - val_loss: 0.2544\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.2441 - val_loss: 0.2540\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2437 - val_loss: 0.2535\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.2433 - val_loss: 0.2531\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.2429 - val_loss: 0.2526\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.2425 - val_loss: 0.2522\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2420 - val_loss: 0.2518\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2416 - val_loss: 0.2513\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.2412 - val_loss: 0.2509\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2408 - val_loss: 0.2504\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2404 - val_loss: 0.2500\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.2400 - val_loss: 0.2496\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.2396 - val_loss: 0.2491\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.2392 - val_loss: 0.2487\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.2388 - val_loss: 0.2483\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2384 - val_loss: 0.2479\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.2380 - val_loss: 0.2474\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2376 - val_loss: 0.2470\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.2372 - val_loss: 0.2466\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.2368 - val_loss: 0.2462\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.2364 - val_loss: 0.2458\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.2360 - val_loss: 0.2453\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.2357 - val_loss: 0.2449\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2353 - val_loss: 0.2445\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2349 - val_loss: 0.2441\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.2345 - val_loss: 0.2437\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2341 - val_loss: 0.2433\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.2337 - val_loss: 0.2429\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.2334 - val_loss: 0.2425\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.2330 - val_loss: 0.2421\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2326 - val_loss: 0.2417\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.2322 - val_loss: 0.2413\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.2319 - val_loss: 0.2409\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.2315 - val_loss: 0.2405\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2311 - val_loss: 0.2401\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.2307 - val_loss: 0.2397\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.2304 - val_loss: 0.2393\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2300 - val_loss: 0.2389\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.2296 - val_loss: 0.2385\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2293 - val_loss: 0.2381\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2289 - val_loss: 0.2377\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.2286 - val_loss: 0.2374\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2282 - val_loss: 0.2370\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.2278 - val_loss: 0.2366\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.2275 - val_loss: 0.2362\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2271 - val_loss: 0.2358\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.2268 - val_loss: 0.2355\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.2264 - val_loss: 0.2351\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.2261 - val_loss: 0.2347\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.2257 - val_loss: 0.2343\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2254 - val_loss: 0.2340\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.2250 - val_loss: 0.2336\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.2247 - val_loss: 0.2332\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.2243 - val_loss: 0.2329\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.2240 - val_loss: 0.2325\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2236 - val_loss: 0.2321\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2233 - val_loss: 0.2318\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2229 - val_loss: 0.2314\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.2226 - val_loss: 0.2310\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.2223 - val_loss: 0.2307\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.2219 - val_loss: 0.2303\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.2216 - val_loss: 0.2299\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.2212 - val_loss: 0.2296\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.2209 - val_loss: 0.2292\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.2206 - val_loss: 0.2289\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2202 - val_loss: 0.2285\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.2199 - val_loss: 0.2282\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2196 - val_loss: 0.2278\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.2192 - val_loss: 0.2275\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.2189 - val_loss: 0.2271\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2186 - val_loss: 0.2268\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2183 - val_loss: 0.2264\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2179 - val_loss: 0.2261\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.2176 - val_loss: 0.2258\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.2173 - val_loss: 0.2254\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2170 - val_loss: 0.2251\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.2167 - val_loss: 0.2247\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.2163 - val_loss: 0.2244\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2160 - val_loss: 0.2241\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2157 - val_loss: 0.2237\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2154 - val_loss: 0.2234\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2151 - val_loss: 0.2231\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.2148 - val_loss: 0.2227\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.2144 - val_loss: 0.2224\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.2141 - val_loss: 0.2221\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2138 - val_loss: 0.2217\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.2135 - val_loss: 0.2214\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.2132 - val_loss: 0.2211\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.2129 - val_loss: 0.2208\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.2126 - val_loss: 0.2204\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.2123 - val_loss: 0.2201\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2120 - val_loss: 0.2198\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.2117 - val_loss: 0.2195\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2114 - val_loss: 0.2191\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.2111 - val_loss: 0.2188\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.2108 - val_loss: 0.2185\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.2105 - val_loss: 0.2182\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2102 - val_loss: 0.2179\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.2099 - val_loss: 0.2175\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.2096 - val_loss: 0.2172\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2093 - val_loss: 0.2169\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.2090 - val_loss: 0.2166\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.2087 - val_loss: 0.2163\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.2084 - val_loss: 0.2160\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.2081 - val_loss: 0.2157\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2078 - val_loss: 0.2154\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.2075 - val_loss: 0.2151\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.2072 - val_loss: 0.2148\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.2070 - val_loss: 0.2145\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.2067 - val_loss: 0.2142\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.2064 - val_loss: 0.2139\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.2061 - val_loss: 0.2136\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.2058 - val_loss: 0.2133\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.2055 - val_loss: 0.2130\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.2052 - val_loss: 0.2127\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2050 - val_loss: 0.2124\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2047 - val_loss: 0.2121\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2044 - val_loss: 0.2118\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.2041 - val_loss: 0.2115\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.2039 - val_loss: 0.2112\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.2036 - val_loss: 0.2110\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.2033 - val_loss: 0.2107\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.2030 - val_loss: 0.2104\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2028 - val_loss: 0.2101\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2025 - val_loss: 0.2098\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.2022 - val_loss: 0.2095\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.2019 - val_loss: 0.2092\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.2017 - val_loss: 0.2090\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2014 - val_loss: 0.2087\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.2011 - val_loss: 0.2084\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2009 - val_loss: 0.2081\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.2006 - val_loss: 0.2078\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.2003 - val_loss: 0.2076\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.2001 - val_loss: 0.2073\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.1998 - val_loss: 0.2070\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1995 - val_loss: 0.2067\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.1993 - val_loss: 0.2065\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.1990 - val_loss: 0.2062\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.1988 - val_loss: 0.2059\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1985 - val_loss: 0.2056\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.1982 - val_loss: 0.2054\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.1980 - val_loss: 0.2051\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.1977 - val_loss: 0.2048\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1975 - val_loss: 0.2046\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.1972 - val_loss: 0.2043\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.1970 - val_loss: 0.2040\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1967 - val_loss: 0.2038\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.1965 - val_loss: 0.2035\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.1962 - val_loss: 0.2033\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.1960 - val_loss: 0.2030\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.1957 - val_loss: 0.2027\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1955 - val_loss: 0.2025\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.1952 - val_loss: 0.2022\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.1950 - val_loss: 0.2020\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.1947 - val_loss: 0.2017\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1945 - val_loss: 0.2015\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.1942 - val_loss: 0.2012\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.1940 - val_loss: 0.2009\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.1937 - val_loss: 0.2007\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.1935 - val_loss: 0.2004\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1932 - val_loss: 0.2002\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.1930 - val_loss: 0.1999\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.1928 - val_loss: 0.1997\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.1925 - val_loss: 0.1994\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1923 - val_loss: 0.1992\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.1921 - val_loss: 0.1990\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.1918 - val_loss: 0.1987\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1916 - val_loss: 0.1985\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.1913 - val_loss: 0.1982\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1911 - val_loss: 0.1980\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.1909 - val_loss: 0.1977\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.1906 - val_loss: 0.1975\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1904 - val_loss: 0.1973\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.1902 - val_loss: 0.1970\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.1899 - val_loss: 0.1968\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.1897 - val_loss: 0.1965\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1895 - val_loss: 0.1963\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.1893 - val_loss: 0.1961\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1890 - val_loss: 0.1958\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.1888 - val_loss: 0.1956\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.1886 - val_loss: 0.1954\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.1884 - val_loss: 0.1951\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.1881 - val_loss: 0.1949\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.1879 - val_loss: 0.1947\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1877 - val_loss: 0.1944\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.1875 - val_loss: 0.1942\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.1872 - val_loss: 0.1940\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.1870 - val_loss: 0.1937\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.1868 - val_loss: 0.1935\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.1866 - val_loss: 0.1933\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1864 - val_loss: 0.1931\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.1861 - val_loss: 0.1928\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.1859 - val_loss: 0.1926\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.1857 - val_loss: 0.1924\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.1855 - val_loss: 0.1922\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1853 - val_loss: 0.1919\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.1851 - val_loss: 0.1917\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.1848 - val_loss: 0.1915\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.1846 - val_loss: 0.1913\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.1844 - val_loss: 0.1911\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.1842 - val_loss: 0.1908\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.1840 - val_loss: 0.1906\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1838 - val_loss: 0.1904\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1836 - val_loss: 0.1903\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1834 - val_loss: 0.1901\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.1833 - val_loss: 0.1901\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.1833 - val_loss: 0.1902\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1834 - val_loss: 0.1903\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.1835 - val_loss: 0.1901\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.1833 - val_loss: 0.1896\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.1828 - val_loss: 0.1889\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.1821 - val_loss: 0.1885\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.1817 - val_loss: 0.1885\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.1817 - val_loss: 0.1886\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.1818 - val_loss: 0.1883\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1815 - val_loss: 0.1878\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.1810 - val_loss: 0.1875\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.1807 - val_loss: 0.1874\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.1806 - val_loss: 0.1873\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.1806 - val_loss: 0.1871\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.1804 - val_loss: 0.1867\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.1800 - val_loss: 0.1864\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.1797 - val_loss: 0.1863\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.1796 - val_loss: 0.1862\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.1795 - val_loss: 0.1860\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.1793 - val_loss: 0.1857\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.1790 - val_loss: 0.1854\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.1787 - val_loss: 0.1853\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.1786 - val_loss: 0.1851\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.1785 - val_loss: 0.1849\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1782 - val_loss: 0.1847\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1780 - val_loss: 0.1844\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.1778 - val_loss: 0.1843\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.1776 - val_loss: 0.1841\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.1775 - val_loss: 0.1839\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.1773 - val_loss: 0.1837\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.1770 - val_loss: 0.1835\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.1768 - val_loss: 0.1833\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.1766 - val_loss: 0.1831\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.1765 - val_loss: 0.1829\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.1763 - val_loss: 0.1827\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.1761 - val_loss: 0.1825\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1759 - val_loss: 0.1823\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.1757 - val_loss: 0.1821\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.1755 - val_loss: 0.1820\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1753 - val_loss: 0.1818\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.1752 - val_loss: 0.1816\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.1750 - val_loss: 0.1814\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.1748 - val_loss: 0.1812\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.1746 - val_loss: 0.1810\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.1744 - val_loss: 0.1808\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.1742 - val_loss: 0.1806\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.1741 - val_loss: 0.1804\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.1739 - val_loss: 0.1802\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.1737 - val_loss: 0.1801\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.1735 - val_loss: 0.1799\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1733 - val_loss: 0.1797\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1731 - val_loss: 0.1795\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1730 - val_loss: 0.1793\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.1728 - val_loss: 0.1791\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.1726 - val_loss: 0.1790\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.1724 - val_loss: 0.1788\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1723 - val_loss: 0.1786\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1721 - val_loss: 0.1784\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.1719 - val_loss: 0.1782\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1717 - val_loss: 0.1781\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.1716 - val_loss: 0.1779\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.1714 - val_loss: 0.1777\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.1712 - val_loss: 0.1775\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.1711 - val_loss: 0.1774\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.1709 - val_loss: 0.1772\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.1707 - val_loss: 0.1770\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.1705 - val_loss: 0.1768\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.1704 - val_loss: 0.1766\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.1702 - val_loss: 0.1765\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.1700 - val_loss: 0.1763\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.1699 - val_loss: 0.1761\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.1697 - val_loss: 0.1759\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.1695 - val_loss: 0.1758\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.1694 - val_loss: 0.1756\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.1692 - val_loss: 0.1754\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.1690 - val_loss: 0.1753\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.1689 - val_loss: 0.1751\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.1687 - val_loss: 0.1749\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.1685 - val_loss: 0.1747\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1684 - val_loss: 0.1746\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.1682 - val_loss: 0.1744\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.1680 - val_loss: 0.1742\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.1679 - val_loss: 0.1741\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.1677 - val_loss: 0.1739\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.1676 - val_loss: 0.1738\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1675 - val_loss: 0.1737\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.1674 - val_loss: 0.1737\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.1674 - val_loss: 0.1739\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.1675 - val_loss: 0.1742\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.1679 - val_loss: 0.1748\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1684 - val_loss: 0.1751\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.1688 - val_loss: 0.1749\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.1686 - val_loss: 0.1737\n"
          ]
        }
      ],
      "source": [
        "unidades_ocultas = 150\n",
        "num_epochs = 1000\n",
        "numero_muestras =train_labels.shape[0]\n",
        "#autoencoder\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Input(shape=(28,28)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(unidades_ocultas, activation='relu', kernel_initializer='HeNormal'))\n",
        "model.add(tf.keras.layers.Dense(area_imagen,kernel_initializer='HeNormal'))\n",
        "model.add(tf.keras.layers.Reshape((28, 28), input_shape=(area_imagen,)))\n",
        "model.summary()\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5) \n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),loss='MeanSquaredError')\n",
        "hist = model.fit(x=train_images, y=train_images, batch_size = numero_muestras, epochs=num_epochs, callbacks = [es],validation_data=(validation_images,validation_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjCi6_nit6hA",
        "outputId": "1a4e93fa-4fc1-4575-e99a-14197364985c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error cuadratico medio de validacion: 0.17365038394927979\n"
          ]
        }
      ],
      "source": [
        "ecm = hist.history['val_loss'][np.shape(hist.history['val_loss'])[0]-1]\n",
        "print(\"Error cuadratico medio de validacion:\", ecm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cuadratic_error(dataset, dataset2):\n",
        "  umbral = np.array([]) #arrays de errores cuadraticos\n",
        "  for x in np.arange(np.shape(dataset)[0]):\n",
        "      a = dataset[x,:,:].reshape(1, 28*28)\n",
        "      b = dataset2[x,:,:].reshape(1,28*28)\n",
        "      ec = np.sum((a-b)**2)\n",
        "      umbral = np.append(umbral, ec)\n",
        "  return umbral"
      ],
      "metadata": {
        "id": "AUjOfELJDKsR"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nuevo\n",
        "new_dataset = np.vstack((validation_images, validation_images2))\n",
        "new_dataset_labels = np.hstack((validation_labels, validation_labels2))\n",
        "predicted_dataset = model.predict(new_dataset)\n",
        "#vector de umbrales en base a su error cuadratico\n",
        "ce_vector = cuadratic_error(new_dataset, predicted_dataset)\n"
      ],
      "metadata": {
        "id": "XfweB2NwA0S3"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(ce_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "E2PPmgn7gpK4",
        "outputId": "e1c25c99-6f99-499a-c3bd-477e08a770d4"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f99ec7a8b90>]"
            ]
          },
          "metadata": {},
          "execution_count": 182
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tp2 = np.sum(ce_vector[:10000]<25)\n",
        "print(tp2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4r67krRo1R1",
        "outputId": "9d7ee53b-591e-472b-8cfa-964ca255e76b"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cant_puntos = 1000\n",
        "umbral = np.linspace(np.min(ce_vector), np.max(ce_vector),cant_puntos)\n",
        "tp = np.zeros(cant_puntos)\n",
        "fn = np.zeros(cant_puntos)\n",
        "tn = np.zeros(cant_puntos)\n",
        "fp = np.zeros(cant_puntos)\n",
        "for (idx,t) in enumerate(umbral):\n",
        "  tp[idx] = np.sum(ce_vector[:10000]<t)\n",
        "  fn[idx] = np.sum(ce_vector[:10000]>t)\n",
        "  tn[idx] = np.sum(ce_vector[10000:]>t)\n",
        "  fp[idx] = np.sum(ce_vector[10000:]<t)\n",
        "\n",
        "tpr = tp/(tp+fn)\n",
        "fpr = fp/(fp+tn)\n",
        "ce_vector2 = (tpr-fpr)**2\n",
        "#eer = \n",
        "plt.title(\"ROC Curve\")\n",
        "plt.xlabel(\"False positive Rate\")\n",
        "plt.ylabel(\"True positive Rate\")\n",
        "plt.xlim(-0.1,1.1)\n",
        "plt.ylim(-0.1,1.1)\n",
        "plt.plot(fpr, tpr, label= 'Roc Curve')\n",
        "plt.plot(0,1,'.' ,label = \"EER\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "SKekdA9mYaLe",
        "outputId": "6b0e7ecc-9005-434e-c1fd-959ce0a7692d"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f99efe61a90>"
            ]
          },
          "metadata": {},
          "execution_count": 186
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nueva base de datos\n",
        "\n",
        "model2 = tf.keras.Sequential()\n",
        "model2.add(tf.keras.layers.Input(shape=(28,28)))\n",
        "model2.add(tf.keras.layers.Flatten())\n",
        "model2.add(tf.keras.layers.Dense(unidades_ocultas, activation='relu', kernel_initializer='HeNormal'))\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAiyjxXWU4Qy",
        "outputId": "b76a19f6-57ca-4145-d1af-3304d8697017"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_6 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 150)               117750    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 117,750\n",
            "Trainable params: 117,750\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.set_weights(model.layers[1].get_weights())\n",
        "dataset3 = np.vstack((train_images, validation_images))\n",
        "predict3 = model2.predict(dataset3)"
      ],
      "metadata": {
        "id": "s-hnENoOt8jM"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#guardo modelos y pesos \n",
        "from keras.models import model_from_json\n",
        "model_json = model.to_json()\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "#serializan los pesos (weights) para HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Modelo guardado en el PC\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2mM2aOAwBJ0",
        "outputId": "fc14aaac-8cae-472c-ce7e-f5b35e474323"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo guardado en el PC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#guardo dataframes\n",
        "df = pd.DataFrame(predict3)\n",
        "df.to_csv('dataframes.csv', index=False)"
      ],
      "metadata": {
        "id": "1y7b24KB0BZY"
      },
      "execution_count": 198,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}